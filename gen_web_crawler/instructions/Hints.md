# CS 1440 Assignment 6: Recursive Web Crawler - Hints

## Students' advice from last semester

*   I went through the starter code and completed the TODO statements one at a time.  Because this assignment was smaller, I jumped into coding with minimal planning. I should've spent more time thinking and planning.
    *   Plan ahead. This one assignment got lots easier once I paused to think it through.
    *   Follow the To-Dos and don't get too fancy.
    *   Understand what the starter code already does and what changes really need to be made to have it function correctly.
*   Play around with the demo code first to understand how it works and then it's easy to figure out.
    *   The REPL is your best friend.Â 
    *   Add comments as needed to clarify your code.

*   I wasted a lot of time testing my program on `https://unnovative.net`.  The right address is `http://unnovative.net`. I wasn't understanding the error I got when I tried to go to that site with `https://`.
    *   Read the Wikipedia article about URLs
    *   After I removed the fragment, it took me a while to figure out that I accidentally left the `#` in the link.
    *   I had to figure out how to use the object returned by `urlparse()`, and also realize that I needed to preserve the original URL string instead of adding all the parsed parts back together. (I'm not saying I couldn't have done something like that. I just realized it added unneeded complexity).
*   Make sure you understand recursion but don't overthink it.
    *   Don't stress too much about it. The word "recursion" seems scary, it really isn't too bad.  Review Erik's notes on recursion if you're stuck.
*   As soon as you get started on it, it'll kind of fall into place. Just be sure to understand the starter code and read through the assignment description to know exactly what you're meant to be doing.
*   Take it slow, test a lot and with different websites of different sizes.
    *   Plan to take some time with the testing server Erik gives you.  Visit it with your browser so you can see what it looks like to your program.


## Erik's Hints

*   Study the supplied demonstration programs to answer your questions about how to use the code libraries.
*   Get an early start on this program so you have enough time to ask questions before the final lecture.
*   Leave yourself plenty of time for testing.
    *   Make sure that your program's command-line interface follows the requirements
    *   Test what happens when you give your program a relative URL
    *   Test what happens when you give your program incorrect crawl depths
*   Identify the base case(s) and handle these at the top of `crawl()`.
    *   *Do not* make a recursive call to `crawl()` when a base case is reached.
    *   Do make a recursive call to `crawl()` otherwise.
*   It is very possible for your program to get into an infinite recursive
    loop.  Watch your program carefully to guard against this!
*   Try [Exception Handling](https://wiki.python.org/moin/HandlingExceptions)
    when you run into errors.
*   You can test your program against `http://cs.usu.edu/` or another website you control.  Some sites consider automated crawlers like this to be a nuisance, or worse.  Be respectful of others' bandwidth.
*   Python functions accept arguments via pass-by-reference.  When your
    `visited` set is modified within a function call, the caller will see that
    its contents have been updated.  This means that you don't need to return
    anything from `crawl()`.
*   If you encounter web pages which cause your crawler to hang, you can avoid
    them by hard-coding their URLs to the `visited` set before your program
    begins.
*   Don't expect that your program's output will exactly match my sample
    output.  What websites you are able to find will depend upon many factors
    outside of our control.  Our program is running loose on the internet,
    which means that it is crawling over a vast network that is constantly
    undergoing change, and which can present different pathways depending upon
    when and how you connect to it.
*   Don't be surprised if your program's output is different today than it was
    yesterday, even though you didn't change anything.
*   Most websites treat URLs with a trailing slash `/` the same as without (for
    example, `http://example.com` vs. `http://example.com/` are the same page).
    Other websites may serve different content from both addresses.  It's
    really not worth the hassle to try to figure this one out.  Google's own
    crawler regards slashed and un-slashed URLs as [different
    pages.](https://webmasters.googleblog.com/2010/04/to-slash-or-not-to-slash.html).
    If it's good enough for Google, it's good enough for your crawler.
*   You may find this [Crawler Test Site](https://crawler-test.com/) to be helpful.


## Using the Testing server

See the instructions in [Using the Testing server](../demo/Using_the_Testing_Server.md).
